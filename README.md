# ml-resources

A place to catalogue some of the interesting learning resources encountered through my ML learning journey.


**Courses**
* [CS 229](https://see.stanford.edu/Course/CS229)
* [CS 329](https://stanford-cs329s.github.io/syllabus.html) - Completed
* [Fullstack Deep Learning](https://fullstackdeeplearning.com/spring2021/) - Completed
* [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
* [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/schedule.html)
  
**Books**
* [The Deep Learning Book](https://www.deeplearningbook.org/) - Completed
* [DL2AI](https://d2l.ai/index.html)
* [Pattern Recognition and Machine Learning](https://www.goodreads.com/book/show/55881.Pattern_Recognition_and_Machine_Learning)
* [Neutral Networks & Deep Learning](http://neuralnetworksanddeeplearning.com/)


**General Blogs**
* [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
* [Schmidhuber 1](https://people.idsia.ch/~juergen/most-cited-neural-nets.html)
* [The Revolution Hasn't Happened Yet](https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7)
* [NLP's ImageNet Moment](https://thegradient.pub/nlp-imagenet/)
* [Why is Machine Learing Hard](http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html)


**General Code/Paper Explanations**
* [LabMLAi](https://nn.labml.ai/index.html)
* [PapersWithCode](https://paperswithcode.com/)


**Neural Networks**
* [Manifold Hypothesis](https://deepai.org/machine-learning-glossary-and-terms/manifold-hypothesis)
* [Colah Manifolds](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
* [Graident Descent Optimizations](https://ruder.io/optimizing-gradient-descent/)
* [Cyclical Learning Rate](https://arxiv.org/pdf/1506.01186.pdf)

**RNN**
* [RNN Effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Attention and Augmented RNNS](https://distill.pub/2016/augmented-rnns/)

**Seq2Seq**
* [Attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)
* [Seq2seq Models with Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
* [Seq2Seq with Attention and Beam](https://guillaumegenthial.github.io/sequence-to-sequence.html)

**LSTMs**
* [Colah's LSTM Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Illustrated Guide to LSTMs and GRUs](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

**CNNs**
* [R-CNN Family](https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html)'
* [Fast-CNN Family](https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html)

**Word Embeddings**
* [Ruder - Word Embedding Series](https://ruder.io/word-embeddings-1/)
* [Stop using Word2Vec - Alternatives](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)

**Transformers**
* [Transformers](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)
* [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* [Illustrated GPT2](http://jalammar.github.io/illustrated-gpt2/)
* [Illustrated GPT3](http://jalammar.github.io/how-gpt3-works-visualizations-animations/)
* [Illustrated BERT](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
* [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)

**Gaussian Mixtures**
* [Brilliant Gaussian Mixture Models](https://brilliant.org/wiki/gaussian-mixture-model/)
* [Python DataSci Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)
  
**Gradient Descent Optimizations**
* [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)

**Entropy**
* [Entropy, Cross-Entropy, KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8&ab_channel=Aur%C3%A9lienG%C3%A9ron)
* [Softmax == Cross-Entropy](https://cs231n.github.io/linear-classify/)

**Dimension Reduction**
* [Tutorial on PCA](https://arxiv.org/abs/1404.1100)

**Classic ML**
* [Baysean Inference](https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348)

**ML Engineering**
* [Intro to PyTorch-Lightning](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09)
* [Google's Rules of ML](https://developers.google.com/machine-learning/guides/rules-of-ml)
* [ML Project Guidelines](https://www.jeremyjordan.me/ml-projects-guide/)
* [Data Science Process Overview](https://docs.microsoft.com/en-us/azure/architecture/data-science-process/overview)
* [Machine Learning Yearning](https://www.goodreads.com/en/book/show/30741739-machine-learning-yearning)
* [Explaining using SHAP Values](https://github.com/slundberg/shap)

**AutoEncoders**
* [Deep Notes Deep Clustering](https://deepnotes.io/deep-clustering)
* [Variational AutoEncoders](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)

**Interesting Questions**
* [What is the reason that a likelihood function is not a pdf?](https://stats.stackexchange.com/questions/31238/what-is-the-reason-that-a-likelihood-function-is-not-a-pdf)
* [What is the difference between "likelihood" and "probability"?](https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability)